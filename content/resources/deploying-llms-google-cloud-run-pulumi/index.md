---
# Name of the event, <= 60 characters
title: Deploy and Manage LLMs on Google Cloud Run GPUs with Pulumi
meta_desc: Deploy and manage LLMs on Google Cloud Run GPUs with Pulumi. Join our workshop to explore scalable, cost-effective infrastructure for fast LLM inference.
meta_image:

# A featured webinar will display first in the list.
featured: false

# Webinars with unlisted as true will not be shown on the webinar list
unlisted: false

# Gated webinars will have a registration form and the user will need
# to fill out the form before viewing.
gated: true

# The layout of the landing page.
type: webinars

# External webinars will link to an external page instead of a webinar
# landing/registration page. If the webinar is external you will need
# set the 'block_external_search_index' flag to true so Google does not index
# the webinar page created.
external: false
block_external_search_index: false

# The url slug for the webinar landing page. If this is an external
# webinar, use the external URL as the value here.
url_slug: deploying-llms-google-cloud-run-pulumi

# Content for the left hand side section of the page.
main:
    # Webinar title.
    title: Deploy and Manage LLMs on Google Cloud Run GPUs with Pulumi

    event_type: workshop # workshop | event

    # URL for embedding a URL for ungated webinars.
    youtube_url: 

    # Sortable date. The datetime Hugo will use to sort the webinars in date order.
    sortable_date: 2024-10-17T09:00:00-07:00

    # Duration of the webinar.
    duration: 90 minutes

    # "virtual" will be shown under "show virtual events only", otherwise shown as City, State (seattle, wa)
    location: virtual

    # Description of the webinar.
    description: |
        If you've ever wanted to inference with an LLM in under a minute while paying only for what you consume, then Google Cloud Run GPUs are for you! In this hands-on workshop, we will demonstrate how Pulumi can seamlessly stand up an environment for deploying your LLMs and custom models on Google Cloud Run GPUs. Participants will learn how to create scalable, cost-efficient infrastructure that allows for rapid LLM inference, leveraging the power of Pulumi to automate and manage their deployments. Whether you're deploying pre-trained LLMs or custom models, this workshop will provide the tools and knowledge you need to optimize your AI workloads on the cloud.

    learn:
        - How to efficiently deploy and manage LLMs and custom models on Google Cloud Run GPUs.
        - Best practices for setting up scalable and cost-effective infrastructure for fast LLM inference.
        - How to automate cloud infrastructure and streamline AI workload management using Pulumi.

    # The webinar presenters
    presenters:
        - name: Jay Smith
          role: Sr. Cloud Customer Engineer, Google
          photo: /images/team/jay-smith-google-jpeg.jpg
        - name: Mitch Gerdisch
          role: Solutions Architect, Pulumi
          photo: /images/team/mitch-gerdisch.jpg

    # case-sensitive
    tags:
        level: Intermediate # Beginner, Intermediate, Advanced
        topics: ["AI"]
        languages: ["Python"]
        clouds: ["Google Cloud"]

# The right hand side form section.
form:
    # HubSpot form id.
    hubspot_form_id: 26493717-a344-4397-bb32-6c381ee73e63
    salesforce_campaign_id: 701PQ00000JPRq5YAH
---

